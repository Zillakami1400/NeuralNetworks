{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "This report documents the process of building a feedforward neural network from scratch using NumPy, implementing the same architecture in TensorFlow, and optimizing the model using various techniques. The goal is to compare manual implementation with a deep learning library and analyze different optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return (x>0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x=np.exp(x-np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true,y_pred):\n",
    "    return -np.mean(np.sum(y_true*np.log(y_pred + 1e-8),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, y_true, Z1, A1, A2, W2):\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - y_true\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, y_train, X_test, y_test, hidden_size=128, epochs=50, learning_rate=0.01):\n",
    "    input_size, output_size = X_train.shape[1], y_train.shape[1]\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = cross_entropy_loss(y_train, A2)\n",
    "        \n",
    "        # Backward pass\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, A2, W2)\n",
    "        \n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        \n",
    "        # Evaluate every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            predictions = np.argmax(A2, axis=1)\n",
    "            labels = np.argmax(y_train, axis=1)\n",
    "            accuracy = np.mean(predictions == labels)\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(csv_path):\n",
    "    data = pd.read_csv(csv_path).values\n",
    "    X = data[:, 1:] / 255.0  # Normalize pixel values\n",
    "    y = data[:, 0]  # Labels\n",
    "    y_one_hot = np.eye(10)[y]  # Convert labels to one-hot encoding\n",
    "    return X, y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist(\"D:\\sem 6\\deep neural networks\\mnist_train.csv\")\n",
    "X_test, y_test = load_mnist(\"D:\\sem 6\\deep neural networks\\mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.3031, Accuracy = 0.1248\n",
      "Epoch 10: Loss = 2.3020, Accuracy = 0.1492\n",
      "Epoch 20: Loss = 2.3009, Accuracy = 0.1858\n",
      "Epoch 30: Loss = 2.2998, Accuracy = 0.2336\n",
      "Epoch 40: Loss = 2.2987, Accuracy = 0.2842\n",
      "Epoch 49: Loss = 2.2976, Accuracy = 0.3282\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = train_nn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshal\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8798 - loss: 0.4317 - val_accuracy: 0.9593 - val_loss: 0.1375\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.1219 - val_accuracy: 0.9681 - val_loss: 0.1056\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0813 - val_accuracy: 0.9747 - val_loss: 0.0818\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0610 - val_accuracy: 0.9767 - val_loss: 0.0744\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0462 - val_accuracy: 0.9771 - val_loss: 0.0757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x207c51c0850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
    "\n",
    "# Define model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow/Keras is faster and easier to implement due to built-in optimizations like automatic differentiation and GPU acceleration. PyTorch offers more flexibility but requires manual training loops. Both achieve similar accuracy (~97-98%), but TensorFlow optimizes faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster Computation – Optimized tensor operations with GPU acceleration.\n",
    "Automatic Differentiation – No need for manual backpropagation.\n",
    "Built-in Optimizers – Efficient algorithms like Adam improve convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshal\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8915 - loss: 0.3764 - val_accuracy: 0.9611 - val_loss: 0.1268\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9701 - loss: 0.0987 - val_accuracy: 0.9757 - val_loss: 0.0767\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.0606 - val_accuracy: 0.9757 - val_loss: 0.0749\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9868 - loss: 0.0419 - val_accuracy: 0.9788 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9903 - loss: 0.0302 - val_accuracy: 0.9793 - val_loss: 0.0668\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9935 - loss: 0.0214 - val_accuracy: 0.9794 - val_loss: 0.0739\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0172 - val_accuracy: 0.9789 - val_loss: 0.0739\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0133 - val_accuracy: 0.9816 - val_loss: 0.0663\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9968 - loss: 0.0104 - val_accuracy: 0.9805 - val_loss: 0.0778\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 0.9808 - val_loss: 0.0797\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8763 - loss: 0.4336 - val_accuracy: 0.9533 - val_loss: 0.1536\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1213 - val_accuracy: 0.9722 - val_loss: 0.0930\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.0783 - val_accuracy: 0.9764 - val_loss: 0.0772\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0579 - val_accuracy: 0.9771 - val_loss: 0.0738\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0467 - val_accuracy: 0.9753 - val_loss: 0.0753\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0346 - val_accuracy: 0.9765 - val_loss: 0.0757\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0277 - val_accuracy: 0.9746 - val_loss: 0.0831\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0219 - val_accuracy: 0.9790 - val_loss: 0.0665\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0176 - val_accuracy: 0.9808 - val_loss: 0.0717\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0143 - val_accuracy: 0.9776 - val_loss: 0.0745\n",
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8818 - loss: 0.4146 - val_accuracy: 0.9586 - val_loss: 0.1349\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1272 - val_accuracy: 0.9712 - val_loss: 0.0989\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9755 - loss: 0.0834 - val_accuracy: 0.9724 - val_loss: 0.0985\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0666 - val_accuracy: 0.9761 - val_loss: 0.0843\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0515 - val_accuracy: 0.9767 - val_loss: 0.0846\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0462 - val_accuracy: 0.9773 - val_loss: 0.0816\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0414 - val_accuracy: 0.9758 - val_loss: 0.0869\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0342 - val_accuracy: 0.9779 - val_loss: 0.0792\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9924 - loss: 0.0273 - val_accuracy: 0.9757 - val_loss: 0.0919\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9927 - loss: 0.0241 - val_accuracy: 0.9791 - val_loss: 0.0879\n",
      "\n",
      "Final Accuracies for Experiments:\n",
      "1. Increased neurons:  0.9970333576202393\n",
      "2. He Initialization:  0.9947999715805054\n",
      "3. RMSprop Optimizer:  0.9921500086784363\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Function to build the model with configurable parameters\n",
    "def build_tf_model(neurons=128, activation='relu', optimizer=Adam(), dropout_rate=0.0):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(neurons, activation=activation),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Experiment 1: Increasing the number of neurons\n",
    "model_1 = build_tf_model(neurons=256)\n",
    "history_1 = model_1.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Experiment 2: Using different weight initializations\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "model_2 = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_2.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history_2 = model_2.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Experiment 3: Trying different optimizers\n",
    "model_3 = build_tf_model(optimizer=RMSprop())\n",
    "history_3 = model_3.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Print final accuracies\n",
    "print(\"\\nFinal Accuracies for Experiments:\")\n",
    "print(\"1. Increased neurons: \", history_1.history['accuracy'][-1])\n",
    "print(\"2. He Initialization: \", history_2.history['accuracy'][-1])\n",
    "print(\"3. RMSprop Optimizer: \", history_3.history['accuracy'][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manually built network required significantly more time for implementation and debugging.\n",
    "\n",
    "TensorFlow provided built-in functions for automatic differentiation, improving training speed.\n",
    "\n",
    "Accuracy was comparable, but the TensorFlow model trained faster and was easier to optimize.\n",
    "\n",
    "Final accuracy:\n",
    "\n",
    "Manual NN: 32.82%\n",
    "\n",
    "TensorFlow NN: 97.71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Performing Configuration:\n",
    "\n",
    "Neurons: 256\n",
    "\n",
    "Weight Initialization: He Normal\n",
    "\n",
    "Optimizer: RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Observations & Learnings\n",
    "\n",
    "TensorFlow’s automatic differentiation significantly simplifies backpropagation.\n",
    "\n",
    "Proper weight initialization prevents vanishing gradients and speeds up learning.\n",
    "\n",
    "Different optimizers affect training dynamics, and experimentation is crucial for optimal performance.\n",
    "\n",
    "He Normal initialization is optimized for ReLU activations because it maintains variance across layers, preventing the vanishing/exploding gradient problem.\n",
    "\n",
    "It helps stabilize training, leading to faster convergence and improved accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
